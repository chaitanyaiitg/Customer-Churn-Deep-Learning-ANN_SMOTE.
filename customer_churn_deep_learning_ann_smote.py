# -*- coding: utf-8 -*-
"""Customer Churn Deep Learning ANN_SMOTE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l_pQKpee8AuwEH1GNvPh8mrYzRlwnx_2
"""

! pip install tensorflow-gpu

! pip install keras

! pip install pandas

"""Customer Churn Prediction Using Artificial Neural Network (ANN)
Customer churn prediction is to measure why customers are leaving a business. In this tutorial we will be looking at customer churn in telecom business. We will build a deep learning model to predict the churn and use precision,recall, f1-score to measure performance of our model
"""

import matplotlib.pylab as plt
import numpy as np
import tensorflow_hub as hub
import tensorflow as tf
print("TF version:", tf.__version__)
print("Hub version:", hub.__version__)
print("GPU is", "available" if tf.test.is_gpu_available() else "NOT AVAILABLE")

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D,MaxPooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
from glob import glob

from sklearn.utils import shuffle
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score
import os
import cv2

from google.colab import drive
drive.mount('/content/drive')

file="/content/drive/My Drive/Colab Notebooks/Customer_Churn/Customer_Churn.csv"

import pandas as pd
df=pd.read_csv(file)
df



df.shape

df.columns

df.isnull().sum()

df.nunique()

df.dtypes

df=df.drop('CustomerID' ,axis=1)

df

df.nunique()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.heatmap(df.isnull())

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df['TotalCharges'].isnull().sum()

df=df[df['TotalCharges'].notnull()]

df

sns.countplot('Churn', data=df)
plt.show()

sns.countplot('Churn', data=df, hue='Gender')
plt.show()

tenure_churn_no = df[df.Churn=='No'].Tenure
tenure_churn_yes = df[df.Churn=='Yes'].Tenure

plt.xlabel("tenure")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")


plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend()

def print_unique_col_values(df):
       for column in df:
            if df[column].dtypes=='object':
                print(f'{column}: {df[column].unique()}')

print_unique_col_values(df)

df.replace('No internet service','No',inplace=True)
df.replace('No phone service','No',inplace=True)

print_unique_col_values(df)

df.dtypes

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df1=df.select_dtypes(include='object')
df1.columns

cols=['Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
       'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
       'PaperlessBilling', 'PaymentMethod', 'Churn']
for i in cols:
  df[i]=encoder.fit_transform(df[i])

df

df.dtypes

labels=df['Churn'].value_counts().keys().tolist()
data=df['Churn'].value_counts().values.tolist()
plt.pie(data,labels=labels,autopct='%.f%%',shadow=True)
plt.show()

cols_to_scale = ['Tenure','MonthlyCharges','TotalCharges']

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

X = df.drop('Churn',axis='columns')
y = df['Churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=10)

X_train.shape

X_test.shape

len(X_train.columns)



model = Sequential([
    Dense(19, input_shape=(19,), activation='relu'),
    Dense(10, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

r=model.fit(X_train, y_train, epochs=200,validation_data = (X_test,y_test))

k=r.history['val_accuracy']
print('The Validation Accuracy of ANN Model: ', np.mean(k))

yp= model.predict(X_test)
yp

y_pred = []
for element in yp:
    if element > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred

y_pred_labels=np.unique(y_pred, return_counts=True)
y_pred_labels

y_test_labels=np.unique(y_test, return_counts=True)
y_test_labels

from sklearn.metrics import confusion_matrix
c_m = confusion_matrix(y_test, y_pred)
c_m

import seaborn as sns
plt.figure(figsize = (6,6))
sns.heatmap(c_m,cmap= "Reds", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' )

from sklearn.metrics import confusion_matrix,roc_curve,auc,accuracy_score
acc_score = accuracy_score(y_test, y_pred)
acc_score

X_test.iloc[0]

b=model.predict([[1,0,0,0,0.422535,1,0,2,0,0,0,0,0,0,2,0,3,0.021891,0.071515]])
b

if(b>0.5):
  print('Churn')
else:
  print('No Churn')

from sklearn.metrics import confusion_matrix , classification_report

print(classification_report(y_test,y_pred))

from imblearn.combine import SMOTETomek
from imblearn.under_sampling import NearMiss
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

os=SMOTETomek()
X_train_ns,y_train_ns=os.fit_sample(X_train,y_train)
print("The number of classes before fit {}".format(Counter(y_train)))
print("The number of classes after fit {}".format(Counter(y_train_ns)))

model1 = Sequential([
    Dense(19, input_shape=(19,), activation='relu'),
    Dense(10, activation='relu'),
    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

rs=model1.fit(X_train_ns, y_train_ns, epochs=200,validation_data = (X_test,y_test))

k1=rs.history['val_accuracy']
print('The Validation Accuracy of ANN Model: ', np.mean(k1))

yp1= model1.predict(X_test)
yp1

y_pred1 = []
for element in yp1:
    if element > 0.5:
        y_pred1.append(1)
    else:
        y_pred1.append(0)

y_pred1

y_pred_labels1=np.unique(y_pred1, return_counts=True)
y_pred_labels1

y_test_labels=np.unique(y_test, return_counts=True)
y_test_labels

from sklearn.metrics import confusion_matrix
c_m1 = confusion_matrix(y_test, y_pred1)
c_m1

import seaborn as sns
plt.figure(figsize = (6,6))
sns.heatmap(c_m1,cmap= "Reds", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' )

from sklearn.metrics import confusion_matrix , classification_report

print(classification_report(y_test,y_pred1))



